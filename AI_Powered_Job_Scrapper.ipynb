{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLWEO0IkOiLd"
      },
      "outputs": [],
      "source": [
        "!pip install gradio #install gradio for Webapp\n",
        "!pip install flair #flair for AI(NLP) skill detection\n",
        "\n",
        "import gradio as gr\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import time\n",
        "import random\n",
        "import threading\n",
        "import os\n",
        "\n",
        "from flair.models import SequenceTagger\n",
        "from flair.data import Sentence\n",
        "\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# Load Flair skill detection model\n",
        "flair_model = SequenceTagger.load('kaliani/flair-ner-skill')\n",
        "\n",
        "# LinkedIn URL mappings\n",
        "experience_level_mapping= {\n",
        "    \"Internship\":\"f_E=1\",\n",
        "    \"Entry level\" : \"f_E=2\",\n",
        "    \"Associate\" : \"f_E=3\",\n",
        "    \"Mid-Senior level\" : \"f_E=4\",\n",
        "}\n",
        "\n",
        "work_type_mapping={\n",
        "    \"On-site\" : \"f_WT=1\",\n",
        "    \"Hybrid\" : \"f_WT=2\",\n",
        "    \"Remote\" : \"f_WT=3\",\n",
        "}\n",
        "\n",
        "time_filter_mapping={\n",
        "    \"Past 24 hours\":\"f_TPR=r86400\",\n",
        "    \"Past week\":\"f_TPR=r604800\",\n",
        "    \"Past month\":\"f_TPR=r2592000\",\n",
        "}\n",
        "\n",
        "# Function to extract skills using Flair\n",
        "def get_skills(text):\n",
        "    sentence = Sentence(text)\n",
        "    flair_model.predict(sentence)\n",
        "    return [entity.text for entity in sentence.get_spans('ner')]\n",
        "\n",
        "# Scraper manager class\n",
        "class ScraperManager:\n",
        "    def __init__(self):\n",
        "        self.stop_event = threading.Event()\n",
        "        self.current_df = pd.DataFrame()\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def reset(self):\n",
        "        self.stop_event.clear()\n",
        "        self.current_df = pd.DataFrame()\n",
        "\n",
        "    def add_job(self, job_data):\n",
        "        with self.lock:\n",
        "            new_df = pd.DataFrame([job_data])\n",
        "            self.current_df = pd.concat([self.current_df, new_df], ignore_index=True)\n",
        "\n",
        "scraper_manager = ScraperManager()\n",
        "\n",
        "# Process individual job\n",
        "def process_job(job, work_type, exp_level, position, tech_keywords=[]):\n",
        "    try:\n",
        "        title_element = job.find('h3', class_='base-search-card__title')\n",
        "        company_element = job.find('a', class_='hidden-nested-link')\n",
        "        loc_element = job.find('span', class_='job-search-card__location')\n",
        "        link_element = job.find('a', class_='base-card__full-link')\n",
        "\n",
        "        if not all([title_element, company_element, loc_element, link_element]):\n",
        "            return None\n",
        "\n",
        "        title = title_element.text.strip()\n",
        "        company = company_element.text.strip()\n",
        "        loc = loc_element.text.strip()\n",
        "        link = link_element['href'].split('?')[0]\n",
        "\n",
        "        session = requests.Session()\n",
        "        retries = Retry(total=3, backoff_factor=1, status_forcelist=[429,500,502,503,504])\n",
        "        session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "\n",
        "        desc = \"Description not available\"\n",
        "        skills = []\n",
        "\n",
        "        try:\n",
        "            time.sleep(random.uniform(2,5))\n",
        "            response = session.get(\n",
        "                link,\n",
        "                headers={\n",
        "                    'User-Agent': random.choice([\n",
        "                        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "                        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',\n",
        "                        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'\n",
        "                    ]),\n",
        "                    'Accept-Language': 'en-Us,en;q=0.9'\n",
        "                },\n",
        "                timeout=10\n",
        "            )\n",
        "\n",
        "            job_soup = BeautifulSoup(response.text,'html.parser')\n",
        "            decription_selectors=[\n",
        "                'div.description__text',\n",
        "                'div.show-more-less-html_markup',\n",
        "                'div.core-section-container__content',\n",
        "                'section.core-section-container'\n",
        "            ]\n",
        "\n",
        "            for selector in decription_selectors:\n",
        "                desc_element = job_soup.select_one(selector)\n",
        "                if desc_element:\n",
        "                    desc = desc_element.get_text('\\n').strip()\n",
        "                    skills = get_skills(desc)\n",
        "                    break\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {link}: {str(e)}\")\n",
        "\n",
        "        # Filter jobs by preferred tech keywords\n",
        "        if tech_keywords:\n",
        "            text_to_check = \" \".join([title, desc] + skills).lower()\n",
        "            if not any(keyword.lower() in text_to_check for keyword in tech_keywords):\n",
        "                return None\n",
        "\n",
        "        return {\n",
        "            \"Position\": position,\n",
        "            \"Date\": datetime.now().strftime('%Y-%m-%d'),\n",
        "            \"Work type\": work_type,\n",
        "            \"Level\": exp_level,\n",
        "            \"Title\": title,\n",
        "            \"Company\": company,\n",
        "            \"Location\": loc,\n",
        "            \"Link\": f\"[{link}]({link})\",\n",
        "            \"Description\": desc,\n",
        "            \"Skills\": \", \".join(skills[:5]) if skills else \"No skills detected\"\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing job card: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Scrape jobs for locations, positions\n",
        "def scrape_jobs(location, position, work_types, exp_levels, time_filter, tech_keywords=[]):\n",
        "    session = requests.Session()\n",
        "    retries = Retry(total=3, backoff_factor=1, status_forcelist=[429,500,502,503,504])\n",
        "    session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "\n",
        "    for work_type in work_types:\n",
        "        for exp_level in exp_levels:\n",
        "            if scraper_manager.stop_event.is_set():\n",
        "                return\n",
        "            try:\n",
        "                base_url = f\"https://www.linkedin.com/jobs/search/?keywords={position}&location={location}\" \\\n",
        "                           f\"&{work_type_mapping[work_type]}\" \\\n",
        "                           f\"&{experience_level_mapping[exp_level]}\" \\\n",
        "                           f\"&{time_filter_mapping[time_filter]}\" \\\n",
        "                           f\"&radius=0\"\n",
        "\n",
        "                try:\n",
        "                    response = session.get(base_url, timeout=10)\n",
        "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                    total_jobs = int(soup.find('span', class_='results-context-header__job-count').text.replace(',',''))\n",
        "                except:\n",
        "                    total_jobs=25\n",
        "\n",
        "                total_jobs=min(total_jobs, 100)\n",
        "\n",
        "                for start in range(0, total_jobs, 25):\n",
        "                    if scraper_manager.stop_event.is_set():\n",
        "                        return\n",
        "                    time.sleep(random.uniform(2,5))\n",
        "                    url = f\"{base_url}&start={start}\"\n",
        "                    try:\n",
        "                        response = session.get(url, timeout=10)\n",
        "                        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                        jobs = soup.find_all('div',class_='base-card')\n",
        "                    except Exception as e:\n",
        "                        print(f\"Failed to scrape page {start}: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "                    random.shuffle(jobs)\n",
        "                    for job in jobs:\n",
        "                        if scraper_manager.stop_event.is_set():\n",
        "                            return\n",
        "                        job_data = process_job(job, work_type, exp_level, position, tech_keywords)\n",
        "                        if job_data:\n",
        "                            scraper_manager.add_job(job_data)\n",
        "                            yield\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Scraping error: {str(e)}\")\n",
        "\n",
        "# Run scraper from Gradio\n",
        "def run_scrapper(cities, states, positions, work_types, exp_levels, time_filter, tech_keywords):\n",
        "    scraper_manager.reset()\n",
        "    cities_list = [c.strip() for c in cities.split(',') if c.strip()]\n",
        "    states_list = [s.strip() for s in states.split(',') if s.strip()]\n",
        "    locations = [f\"{city}, {state}\" for city in cities_list for state in states_list]\n",
        "    positions_list = [p.strip().replace(' ','%20') for p in positions.split(',') if p.strip()]\n",
        "    tech_keywords_list = [t.strip() for t in tech_keywords.split(',') if t.strip()]\n",
        "\n",
        "    def worker():\n",
        "        for loc in locations:\n",
        "            for pos in positions_list:\n",
        "                if scraper_manager.stop_event.is_set():\n",
        "                    return\n",
        "                for _ in scrape_jobs(loc, pos, work_types, exp_levels, time_filter, tech_keywords_list):\n",
        "                    pass\n",
        "\n",
        "    thread = threading.Thread(target=worker)\n",
        "    thread.start()\n",
        "\n",
        "    while thread.is_alive():\n",
        "        time.sleep(0.5)\n",
        "        with scraper_manager.lock:\n",
        "            yield 'Scraping in progress...', scraper_manager.current_df\n",
        "\n",
        "    yield \"Scraping Completed!\" if not scraper_manager.stop_event.is_set() else \"Scraping stopped!\", scraper_manager.current_df\n",
        "\n",
        "# Save to CSV\n",
        "def save_csv(df, filename):\n",
        "    if filename.strip() == \"\":\n",
        "        filename = \"my_jobs\"\n",
        "    file_path = f\"{filename}.csv\"\n",
        "    try:\n",
        "        df.to_csv(file_path, index=False)\n",
        "        return f\"Saved as {file_path}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error saving CSV: {str(e)}\"\n",
        "\n",
        "# Gradio App\n",
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"\"\"\n",
        "    <div style='text-align: center; color: #f67d3c; font-size: 2em; font-weight: bold; margin: 20px 0; padding: 10px;'>\n",
        "        AI-Powered Linkedin Job Scraper\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            cities = gr.Textbox(label=\"Cities (comma-separated)\")\n",
        "            states = gr.Textbox(label=\"States/Countries (comma-separated)\")\n",
        "            positions = gr.Textbox(label=\"Positions (comma-separated)\")\n",
        "            work_types = gr.CheckboxGroup(list(work_type_mapping.keys()), label=\"Work Types\")\n",
        "            exp_levels = gr.CheckboxGroup(list(experience_level_mapping.keys()), label=\"Experience Levels\")\n",
        "            time_filter = gr.Dropdown(list(time_filter_mapping.keys()), label=\"Time Filter\")\n",
        "            tech_keywords = gr.Textbox(label=\"Preferred Technologies/Fields (comma-separated)\", placeholder=\"Python, ML, AI\")\n",
        "\n",
        "            with gr.Row():\n",
        "                start_btn = gr.Button(\"Start Scraping\", variant=\"primary\")\n",
        "                stop_btn = gr.Button(\"Stop Scraping\", variant=\"secondary\")\n",
        "\n",
        "        status = gr.Textbox(label=\"Status\")\n",
        "        results = gr.Dataframe(\n",
        "            headers = [\"Position\", \"Date\", \"Work type\", \"Level\", \"Title\", \"Company\", \"Location\", \"Link\", \"Skills\"],\n",
        "            datatype = [\"str\",\"str\",\"str\",\"str\",\"str\",\"str\",\"str\",\"str\",\"str\"],\n",
        "            interactive = False\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            filename = gr.Textbox(label=\"Filename (optional)\", placeholder=\"my_jobs\")\n",
        "            save_btn = gr.Button(\"Save to csv\", variant=\"secondary\")\n",
        "            save_status = gr.Textbox(label=\"Save status\")\n",
        "\n",
        "        start_btn.click(\n",
        "            run_scrapper,\n",
        "            inputs=[cities, states, positions, work_types, exp_levels, time_filter, tech_keywords],\n",
        "            outputs=[status, results]\n",
        "        )\n",
        "\n",
        "        stop_btn.click(\n",
        "            lambda: scraper_manager.stop_event.set(),\n",
        "            outputs=[]\n",
        "        )\n",
        "\n",
        "        save_btn.click(\n",
        "            save_csv,\n",
        "            inputs=[results, filename],\n",
        "            outputs=save_status\n",
        "        )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.launch()\n"
      ]
    }
  ]
}